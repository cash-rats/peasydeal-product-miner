# RabbitMQ Crawl Worker (Async, No Retry) — Feasibility + Implementation Plan

## Goal

Implement the same business flow currently in `internal/app/inngest/crawl/crawl.go` (check devtools → resolve out dir → run crawler → persist product draft), but triggered and executed via RabbitMQ (AMQP) in a **separate worker binary**, running **asynchronously**, with **no application-level retries**.

## Non-goals

- Recreating Inngest step observability (per-step UI), orchestration, or durable step checkpoints.
- Building a general workflow engine.
- Adding retry/backoff logic (TTL queues, delayed retries, etc.).

## Current flow (Inngest reference)

`internal/app/inngest/crawl/crawl.go` currently does:

1) `check-devtools`
   - Calls `chromedevtools.CheckReachable(...)`.
   - On failure: returns `inngestgo.NoRetryError(err)` and **does not persist** a product draft.

2) `resolve-out-dir`
   - Defaults `out_dir` to `"out"` when empty.

3) `run-crawler`
   - Calls `runner.RunOnce(...)`.
   - On crawler error: **does not fail the step**; it returns a result shaped as failure so persistence can still happen.

4) `persist-product-draft`
   - Calls `ProductDraftStore.UpsertFromCrawlResult(...)` and returns `draft_id`.
   - On failure: returns `NoRetryError` (hard-fail).

## Feasibility (RabbitMQ)

This is feasible with minimal risk because:

- The flow is already expressed as a single function with clear boundaries and error semantics.
- RabbitMQ fits the “event triggers a long-running job” model; a worker can run `runner.RunOnce` out-of-band.
- The repo already uses FX for lifecycle wiring; RabbitMQ connection/channel can be cleanly managed via `fx.Lifecycle`.

What RabbitMQ does *not* provide (vs Inngest) and we must compensate for:

- Step-level tracing/checkpointing: RabbitMQ delivers messages; it does not track “step runs”.
- Exactly-once delivery: RabbitMQ is **at-least-once**. We must implement idempotency to tolerate duplicates.

## Message contract (v1)

### Decisions (contract + routing)

- **Exchange type:** `topic`
- **Routing key (v1):** `crawler.url.requested.v1`
- **Payload `event_name`:** `crawler/url.requested` (same as `CrawlRequestedEventName`)
- **Id key:** `event_id` (required) and mirrored to AMQP `MessageId`
- **Timestamp format:** RFC3339 (UTC recommended)

### JSON payload

```json
{
  "event_name": "crawler/url.requested",
  "event_id": "uuid",
  "ts": "2026-01-27T12:34:56Z",
  "data": {
    "url": "https://example.com/p/123",
    "out_dir": "out"
  }
}
```

Notes:

- `event_id` is **required**. Messages without `event_id` are treated as invalid and sent to DLQ.
- `event_id` MUST be generated by the producer (UUID) and treated as the idempotency key.
- `out_dir` is optional; worker defaults to `"out"` when empty (same behavior as Inngest).

### AMQP properties (recommended)

- `ContentType = "application/json"`
- `MessageId = event_id`
- `Type = "crawler/url.requested"`
- `DeliveryMode = 2` (persistent)

## RabbitMQ topology (minimal, no retry)

Even with “no retry”, a DLQ is valuable so failures are not silently lost.

- Exchange: `events` (type: `topic`)
- Queue: `crawler.url.requested.v1`
- Binding: routing key `crawler.url.requested.v1` (recommended) OR exact `crawler/url.requested` (works, but topic wildcards only operate on `.` segments)
- DLX/DLQ:
  - DLX: `events.dlx` (type: `topic`)
  - DLQ: `crawler.url.requested.v1.dlq`
  - Main queue arguments: `x-dead-letter-exchange=events.dlx`

**No application-level retry rule:**

- Worker never requeues (`Nack(requeue=true)` is not used).
- Failures are `Reject(requeue=false)` (or `Nack(requeue=false)`) and routed to DLQ.

Important nuance: if the worker process crashes after receiving but before acking, RabbitMQ will redeliver. That’s unavoidable and is why idempotency is required.

## Worker: exact flow mapping

### Overview

The worker consumes a crawl-request message and runs the same steps sequentially.

**Ack strategy:** ACK only after persistence succeeds (or after we have intentionally decided not to persist, which should match current Inngest behavior).

### Step-by-step behavior

1) **Parse + validate**
   - Unmarshal JSON; trim URL.
   - If URL is empty/invalid JSON: `Reject(requeue=false)` → DLQ.

2) **check-devtools**
   - Call `chromedevtools.VersionURLResolved(...)` + `chromedevtools.CheckReachable(...)`.
   - If unreachable: `Reject(requeue=false)` → DLQ.
   - Match Inngest semantics: **do not persist** a product draft on this failure.

3) **resolve-out-dir**
   - If empty, set to `"out"`.

4) **run-crawler**
   - Call `runner.RunOnce(...)`.
   - If error: log `inngest_crawl_failed`-equivalent and continue with the failure-shaped result (same as current behavior).

5) **persist-product-draft**
   - Call `ProductDraftStore.UpsertFromCrawlResult(...)` with:
     - `EventID = event_id` (from AMQP `MessageId` / payload)
     - `URL = url`
     - `Result = runner.Result`
   - If persistence fails: `Reject(requeue=false)` → DLQ.
   - If persistence succeeds: `Ack()`.

## Idempotency & duplicates (required)

RabbitMQ delivery is at-least-once; duplicates can happen (redelivery, producer retries, operator requeue from DLQ).

### Decision: idempotent upsert by `event_id` (stored in SQLite)

We will treat `event_id` as the idempotency key and **store it in SQLite** as `product_drafts.event_id` with a UNIQUE index.

Planned persistence semantics:

- Worker passes `EventID = event_id` into `ProductDraftStore.UpsertFromCrawlResult(...)`.
- Storage writes `event_id` to a new `product_drafts.event_id` column and uses `ON CONFLICT(event_id) DO UPDATE ...`.
- Storage returns the stable `product_drafts.id` (a draft UUID), so callers can reference drafts independently of the message/event id.

### Planned schema change (Turso SQLite)

- Add column: `ALTER TABLE product_drafts ADD COLUMN event_id TEXT;`
- Add unique index: `CREATE UNIQUE INDEX IF NOT EXISTS idx_product_drafts_event_id ON product_drafts(event_id);`
  - SQLite UNIQUE indexes allow multiple NULLs; we will require `event_id` at the application layer for this flow.

Note: the current store implementation uses `event_id` as `product_drafts.id`; it will be updated as part of this feature.

## FX wiring plan (separate worker binary)

### New entrypoint

- Add `cmd/worker/main.go` which starts a long-lived FX app, similar to `cmd/server/main.go`, but without the HTTP router/server modules.

### Modules (planned)

Reuse existing providers where possible:

- `internal/app/fx.CoreAppOptions` (viper + typed config + logger)
- `db/fx.SQLiteModule` (Turso/libSQL sqlite connection for `ProductDraftStore`)
- Runner wiring (same as today’s Inngest module):
  - `internal/runner/fx` runners + `runner.NewRunner`
- New AMQP module(s):
  - `internal/pkg/amqpclient` (connection/channel constructor + lifecycle close hooks; no global singletons)
  - `internal/app/amqp/crawlworker` (consumer that invokes the crawl flow; started via `fx.Invoke` + `fx.Lifecycle`)

### Shutdown semantics

- OnStart:
  - connect to RabbitMQ
  - declare exchange/queue/bindings (or assume they exist; decide once)
  - start consuming with manual ack and `prefetch=1` (start conservative)
- OnStop:
  - cancel consume context
  - close channel/connection

## Producer plan (server-side, async enqueue)

Deferred. For now, assume crawl-request messages are published by an external producer (manual publish, script, or another service).

When we add a first-class producer later, the recommended approach is an HTTP endpoint in the server process that publishes `crawler/url.requested` and returns `202 { event_id }`.

## Local development plan

- Add a `rabbitmq` service to `docker-compose.yml` (or a separate `docker-compose.dev.yml`).
- Add a `worker` service that runs `/app/worker` with the same volumes as `server` for `/out` if file output is needed.
- Minimal env vars (proposed):
  - `RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/`
  - `RABBITMQ_EXCHANGE=events`
  - `RABBITMQ_QUEUE=crawler.url.requested.v1`
  - `RABBITMQ_ROUTING_KEY=crawler.url.requested.v1`
  - `RABBITMQ_PREFETCH=1`
  - `RABBITMQ_DECLARE_TOPOLOGY=true`

### Config keys + defaults

RabbitMQ config lives in `config/config.go` under `Config.RabbitMQ` (FX-injected as `*config.Config`), using these keys (and their corresponding env vars):

- `rabbitmq.url` (`RABBITMQ_URL`): default `""` (disabled)
- `rabbitmq.exchange` (`RABBITMQ_EXCHANGE`): default `events`
- `rabbitmq.queue` (`RABBITMQ_QUEUE`): default `crawler.url.requested.v1`
- `rabbitmq.routing_key` (`RABBITMQ_ROUTING_KEY`): default `crawler.url.requested.v1`
- `rabbitmq.prefetch` (`RABBITMQ_PREFETCH`): default `1`
- `rabbitmq.declare_topology` (`RABBITMQ_DECLARE_TOPOLOGY`): default `true`

## Error policy summary (exactness checklist)

- Missing URL / invalid JSON: reject, no requeue, DLQ.
- Devtools unreachable: reject, no requeue, DLQ; **no draft persisted**.
- Crawler fails: **persist failure** to product drafts; ack after persistence.
- Persistence fails: reject, no requeue, DLQ.

## Acceptance criteria

- Publishing one crawl request message results in:
  - one worker consumption
  - one call to `runner.RunOnce`
  - one `product_drafts` upsert (success or failure) keyed by `event_id`
- When `chromedevtools.CheckReachable` fails, message ends in DLQ and no draft row is written (matches current behavior).
- Redelivering the same message (same `event_id`) does not create duplicate drafts (idempotent upsert).

## Open questions to confirm before coding

1) **Decision:** worker declares topology (exchange/queue/bindings, and DLX/DLQ if used) on startup via idempotent `ExchangeDeclare`/`QueueDeclare`/`QueueBind`. (We can add an env flag later to disable this in production if you want strict pre-provisioning / least-privilege.)
2) Should DLQ messages include additional metadata (error string, stack, worker version) via headers (recommended) or rely on logs only?
3) Is the existing `ProductDraftStore` intended to be Inngest-specific, or should it be moved to a non-Inngest domain package before wiring the worker?

## TODO (progress tracker)

- [x] Confirm RabbitMQ topology decision: worker declares exchange/queue/bindings vs pre-provisioned
- [x] Confirm message contract v1 (JSON fields, `event_id` source, routing key naming)
- [x] Confirm idempotency rule: `event_id` must upsert (no duplicates) (via `product_drafts.event_id` UNIQUE)
- [x] Add AMQP config fields (optional env vars; defaults keep worker startable)
- [ ] Create `internal/pkg/amqpclient` with FX-provided connection/channel + `fx.Lifecycle` close hooks
- [ ] Create `internal/app/amqp/crawlworker` consumer module (manual ack, `prefetch=1`, DLQ on failures)
- [ ] Implement worker “handler” that maps steps exactly: validate → devtools check (no persist on fail) → resolve out_dir → run crawler (persist failures) → persist draft (ack only after success)
- [ ] Add Turso migration: `product_drafts.event_id` column + UNIQUE index
- [ ] Update `ProductDraftStore.UpsertFromCrawlResult` to upsert on `event_id` and return stable `draft_id`
- [ ] Add `cmd/worker/main.go` entrypoint wiring CoreAppOptions + SQLiteModule + runner providers + crawlworker module
- [ ] Add docker-compose worker service (and rabbitmq service if not already) for local dev
- [ ] Add minimal run docs (env vars, `make` target or `go run ./cmd/worker`)
- [ ] Add a basic integration test or harness (optional) to publish one message and assert a draft row is written (skip if test infra too heavy)
- [ ] (Later) Add producer HTTP endpoint to publish crawl requests and return `202 {event_id}`
